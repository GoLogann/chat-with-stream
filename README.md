```markdown
# Chat With Stream (FastAPI + LangChain + AWS Bedrock)

Este projeto Ã© uma API **FastAPI** que expÃµe um **WebSocket** para chat em **streaming** com modelos do **AWS Bedrock** via **LangChain**.  
A resposta do modelo Ã© enviada **token a token em tempo real** para o cliente conectado.

---

## ğŸš€ Tecnologias

- [FastAPI](https://fastapi.tiangolo.com/) â€” Web framework assÃ­ncrono.
- [LangChain](https://www.langchain.com/) â€” OrquestraÃ§Ã£o de LLMs.
- [AWS Bedrock](https://aws.amazon.com/bedrock/) â€” Modelos de linguagem gerenciados.
- [Dependency Injector](https://python-dependency-injector.ets-labs.org/) â€” InjeÃ§Ã£o de dependÃªncia.
- WebSocket â€” Canal de comunicaÃ§Ã£o bidirecional em tempo real.
- Docker â€” ContainerizaÃ§Ã£o da aplicaÃ§Ã£o.

---

## ğŸ“‚ Estrutura do Projeto

```

.
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ adapter
â”‚   â”‚   â”œâ”€â”€ routers           # Rotas WebSocket
â”‚   â”‚   â””â”€â”€ schemas           # Schemas Pydantic
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ config.py         # ConfiguraÃ§Ãµes da aplicaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ exceptions.py     # ExceÃ§Ãµes customizadas
â”‚   â”‚   â””â”€â”€ service
â”‚   â”‚       â”œâ”€â”€ chat          # ChatService (fachada)
â”‚   â”‚       â””â”€â”€ llm           # ServiÃ§os LangChain/Bedrock
â”‚   â”œâ”€â”€ container.py          # Container de dependÃªncias
â”œâ”€â”€ main.py                   # EntryPoint FastAPI/Uvicorn
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â””â”€â”€ README.md

````

---

## âš™ï¸ ConfiguraÃ§Ã£o

Crie um arquivo `.env` com suas credenciais AWS:

```env
AWS_PROFILE=default
AWS_REGION=us-east-1
BEDROCK_MODEL_ID=amazon.nova-micro-v1:0
TEMPERATURE=0.3
PORT=8000
````

---

## â–¶ï¸ Rodando localmente

### 1. Ambiente virtual

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Executar servidor

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

---

## ğŸ³ Rodando com Docker

```bash
docker build -t chat-with-stream .
docker run -it --rm -p 8000:8000 --env-file .env chat-with-stream
```

---

## ğŸ’¬ Uso via WebSocket

### URL

```
ws://localhost:8000/ws/chat/completions
```

### Mensagem inicial (JSON)

```json
{
  "question": "Explique resumidamente o que Ã© LangChain.",
  "session_id": "sessao-teste-1"
}
```

### Resposta esperada

```json
{"type":"start","session_id":"sessao-teste-1"}
{"type":"token","text":"LangChain"}
{"type":"token","text":" Ã© um framework..."}
{"type":"end","full_text":"LangChain Ã© um framework..."}
```

### Encerramento automÃ¡tico

* Se o cliente fechar o WebSocket (reload/aba fechada), o backend encerra a sessÃ£o.
