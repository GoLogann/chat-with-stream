# Chat With Stream (FastAPI + LangChain + AWS Bedrock)

Este projeto Ã© uma API **FastAPI** que expÃµe um **WebSocket** para chat em **streaming** com modelos do **AWS Bedrock** via **LangChain**.  
A resposta do modelo Ã© enviada **token a token em tempo real** para o cliente conectado.

---

## ğŸš€ Tecnologias

- [FastAPI](https://fastapi.tiangolo.com/) â€” Web framework assÃ­ncrono
- [LangChain](https://www.langchain.com/) â€” OrquestraÃ§Ã£o de LLMs
- [AWS Bedrock](https://aws.amazon.com/bedrock/) â€” Modelos de linguagem gerenciados
- [Dependency Injector](https://python-dependency-injector.ets-labs.org/) â€” InjeÃ§Ã£o de dependÃªncia
- WebSocket â€” Canal de comunicaÃ§Ã£o bidirecional em tempo real
- Docker â€” ContainerizaÃ§Ã£o da aplicaÃ§Ã£o

---

## ğŸ“‚ Estrutura do Projeto

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ adapter/
â”‚   â”‚   â”œâ”€â”€ routers/          # Rotas WebSocket
â”‚   â”‚   â””â”€â”€ schemas/          # Schemas Pydantic
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py         # ConfiguraÃ§Ãµes da aplicaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ exceptions.py     # ExceÃ§Ãµes customizadas
â”‚   â”‚   â””â”€â”€ service/
â”‚   â”‚       â”œâ”€â”€ chat/         # ChatService (fachada)
â”‚   â”‚       â””â”€â”€ llm/          # ServiÃ§os LangChain/Bedrock
â”‚   â””â”€â”€ container.py          # Container de dependÃªncias
â”œâ”€â”€ main.py                   # EntryPoint FastAPI/Uvicorn
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â””â”€â”€ README.md
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

Crie um arquivo `.env` na raiz do projeto com suas credenciais AWS:

```env
AWS_PROFILE=default
AWS_ACCOUNT_ID=xxxyyy
AWS_REGION=us-east-1
BEDROCK_MODEL_ID=amazon.nova-micro-v1:0
TEMPERATURE=0.3
PORT=8000
```

---

## â–¶ï¸ Executando Localmente

### 1. Preparar ambiente virtual

```bash
python3 -m venv .venv
source .venv/bin/activate  # No Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Executar o servidor

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

A API estarÃ¡ disponÃ­vel em: `http://localhost:8000`

---

## ğŸ³ Executando com Docker

```bash
# Build da imagem
docker build -t chat-with-stream .

# Executar container
docker run -it --rm -p 8000:8000 --env-file .env chat-with-stream
```

---

## ğŸ’¬ Como Usar o WebSocket

### Endpoint
```
ws://localhost:8000/ws/chat/completions
```

### Enviando uma mensagem (JSON)
```json
{
  "question": "Explique resumidamente o que Ã© LangChain.",
  "session_id": "sessao-teste-1"
}
```

### Formato das respostas
O servidor enviarÃ¡ uma sequÃªncia de mensagens JSON:

```json
{"type": "start", "session_id": "sessao-teste-1"}
{"type": "token", "text": "LangChain"}
{"type": "token", "text": " Ã© um framework"}
{"type": "token", "text": " para desenvolvimento..."}
{"type": "end", "full_text": "LangChain Ã© um framework para desenvolvimento..."}
```

### Tipos de resposta
- `start` â€” InÃ­cio da resposta
- `token` â€” Token individual da resposta (streaming)
- `end` â€” Fim da resposta com texto completo
- `error` â€” Erro durante o processamento

### Comportamento da sessÃ£o
- Se o cliente fechar o WebSocket (reload/aba fechada), o backend encerra automaticamente a sessÃ£o
- Cada `session_id` mantÃ©m o contexto da conversa

---

## ğŸ”§ Desenvolvimento

### Estrutura de arquitetura
- **Adapter Layer**: Rotas e schemas de entrada/saÃ­da
- **Core Layer**: LÃ³gica de negÃ³cio e serviÃ§os
- **Service Layer**: IntegraÃ§Ã£o com LangChain e AWS Bedrock
- **Container**: Gerenciamento de dependÃªncias

### Principais componentes
- `ChatService`: Fachada principal para operaÃ§Ãµes de chat
- `LLMService`: IntegraÃ§Ã£o com modelos do AWS Bedrock
- `WebSocketRouter`: Gerenciamento de conexÃµes WebSocket

---

## ğŸ“ Notas

- Certifique-se de ter as credenciais AWS configuradas corretamente
- O modelo padrÃ£o Ã© o `us.anthropic.claude-3-7-sonnet-20250219-v1:0`, mas pode ser alterado no `.env`
- A temperatura padrÃ£o Ã© `0.3` para respostas mais determinÃ­sticas
